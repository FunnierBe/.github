# Recursively Neural 

*Funny? Funny, how? In what way? Why is that funny? Is this some kind of a joke?*

That which we cannot joke about, we must pass over in silence.

1. [Transformers](https://github.com/cedrickchee/awesome-transformer-nlp)

* [Encoder-decoder framework](https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b)
* [Attention mechanisms](https://github.com/xmu-xiaoma666/External-Attention-pytorch#attention-series)
* [Transfer learning](https://github.com/artix41/awesome-transfer-learning)
2. Text Classification
* [Datasets](https://huggingface.co/datasets)
* [Tokenizers](https://github.com/topics/tokenizer) for Sub-Words, Words, [Sentences](https://github.com/google/sentencepiece), Paragraphs, Documents
* [Text Analytics](https://github.com/dipanjanS/text-analytics-with-python)

3. Transformer Anatomy

* Getting the embedded vectors or the hidden state or context
* Iterative generation, brainstorming, riffing without the meeting hogs, clowns, attention whores

4. Calling Out Multijargon Stupid-Named Entity misRecognition

* Double entrendre
* Ambiguity
* Puns, malapropisms
* Finer points
* Distinction
* No, you don't understand ... you think you do ... but you DON'T understand.

5. Joke Generation

6. Bad Summarization and Missing The Point To Make A Point

7. Question Misanswering and Dodging

8. Making Transformers Funny In Production

9. Dealing With Few To No Labels

10. Training Transformers From Scratch

11. Future Directions
