# Recursively Neural 

*Funny? Funny, how? In what way? Why is that funny?*

That which we cannot joke about, we must pass over in silence.

1. [Transformers](https://github.com/cedrickchee/awesome-transformer-nlp)

* [Encoder-decoder framework](https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b)
* [Attention mechanisms](https://github.com/xmu-xiaoma666/External-Attention-pytorch#attention-series)
* [Transfer learning](https://github.com/artix41/awesome-transfer-learning)
2. Text Classification
* [Datasets](https://huggingface.co/datasets)
* [Tokenizers](https://github.com/topics/tokenizer) for Sub-Words, Words, [Sentences](https://github.com/google/sentencepiece), Paragraphs, Documents
* [Text Analytics](https://github.com/dipanjanS/text-analytics-with-python)

3. Transformer Anatomy

* Getting the embedded vectors or the hidden state or context
* Iterative generation, brainstorming, riffing without the meeting hogs, clowns, attention whores

4. Multijargon Named Entity Recognition

* Double entrendre
* Ambiguity
* Puns, malapropisms
* Finer points
* Distinction

5. Joke Generation

6. Bad Summarization

7. Question Misanswering

8. Making Transformers Funny In Production

9. Dealing With Few To No Labels

10. Training Transformers From Scratch

11. Future Directions
